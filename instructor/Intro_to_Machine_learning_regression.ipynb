{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "name": "python", "version": "3.5.4"}, "anaconda-cloud": {}}, "nbformat": 4, "cells": [{"metadata": {}, "cell_type": "markdown", "source": ["<a id='top'></a>\n", "# Log completion by ML regression"]}, {"metadata": {}, "cell_type": "markdown", "source": ["- Typical and useful Pandas\n", "    - Data exploration using Matplotlib\n", "    - Basic steps for data cleaning\n", "    - **Exercise: Find problem in specific well log data.**\n", "    - Feature engineering\n", "- Setup scikit-learn workflow\n", "    - Making X and y\n", "- Choosing a model\n", "    - Classification vs Regression\n", "- Evaluating model performance\n", "    - Parameter selection and tuning\n", "    - GridSearch\n", "- Add more data / remove data "]}, {"metadata": {}, "cell_type": "markdown", "source": ["## More Pandas\n", "---"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Load Numpy, Pandas and Matplotlib"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "% matplotlib inline"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["Define the name of the file to be loaded and use Pandas to read it. Note that the name can be a PATH pointing at the file."]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["datafile = '../data/training_DataFrame.csv'"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["Pandas expects by default a column on the file to be an index for each row of values. For this example, column 1 (index = 0) is that column."]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["wells = pd.read_csv(datafile, index_col=0)"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["# Data Exploration and cleaning"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Before feeding our machines with data to learn from, it's important to make sure that we feed them the best possible data. Pandas has a few methods to explore the contents of the data. The `head()` method shows the top rows of the DataFrame."]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["wells.head()"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["Another useful Pandas method is `describe()`, which compile useful statistics of each numeric column in the `DataFrame`. "]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["wells.describe()"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["Note how the `count` row is not the same for all columns? This means that there are some values that Pandas doesn't think they are numbers! (Could be missing values or `NaN`s). There are many strategies to deal with missing data but for this excercise we're just going to ignore the rows that contain these bad values."]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["wells = wells.dropna()"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["wells.describe()"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["Now every column in the `DataFrame` should contain the same number of elements and now we can focus on the statistics themselves. Look at each log property, do those `mean`, `min` and `max` look OK? `ILD` shouldn't have negative values. Let's take them out of our set:"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["wells = wells[wells.ILD > 0]"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["wells.describe()"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["Another typical first approach to explore the data is to study the distribution of values in the dataset..."]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["ax = wells.hist(column=\"RHOB\", figsize=(8,6), bins=20)"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["<div class=\"alert alert-success\">\n", "    <b>Exercise</b>:\n", "     <ul>\n", "      <li>\n", "      That distribution doesn't seem right. Can you exclude the `DataFrame` values for which `RHOB` is higher than `1800`?\n", "      </li>\n", "      <p>\n", "    </ul>\n", "</div>"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["# Put your code here\n", "#!--\n", "wells = wells[wells.RHOB > 1800]\n", "#--!"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["<div class=\"alert alert-success\">\n", "    <b>Exercise</b>:\n", "     <ul>\n", "      <li>\n", "      Explore the rest of the `DataFrame`. Do all distributions look OK?\n", "      </li>\n", "      <p>\n", "    </ul>\n", "</div>"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Seaborn has a few tricks to display histograms better"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["import seaborn as sns"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["wells.ILD.values"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["sns.distplot(wells['ILD'])"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["<div class=\"alert alert-success\">\n", "    <b>Exercise</b>:\n", "     <ul>\n", "      <li>\n", "      Calculate the `log` of ILD and store it in the `DataFrame`\n", "      </li>\n", "      <p>\n", "    </ul>\n", "</div>"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["# Put your code here\n", "#!--\n", "wells['log_ILD'] = np.log10(wells['ILD'])\n", "axs = wells['log_ILD'].hist(bins=20)\n", "#--!"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["wells = wells[wells.DPHI > 0]"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["sns.distplot(wells.DPHI)"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["# Load testing data"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["w_train = wells.copy()\n", "w_test = pd.read_csv('../data/testing_DataFrame.csv', index_col=0)\n", "w_test_complete = pd.read_csv('../data/testing_DataFrame_complete.csv', index_col=0)"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["w_test.head()"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["w_test.describe()"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["w_test = w_test[w_test.DPHI > 0]"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["w_test_complete = w_test_complete[w_test_complete.DPHI > 0]"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["w_test.describe()"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["Let's start testing our training pipeline with a subset of wells. We can come back to this and change the number of wells we include, to see how it affects the result."]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["w_train = w_train[w_train.well_ID < 25]"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["# Make X and y\n", "X = w_train[['Depth','GR','ILD','NPHI']].as_matrix()\n", "y = w_train['RHOB'].values"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["X.shape"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["Set up the testing matrix of features we want to use to predict the missing `RHOB`"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["X_test = w_test[['Depth','GR','ILD','NPHI']].as_matrix()"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["We will display the predicted vs. true results for a test well"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["well_id = 81"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["# Available scikit-learn models to choose from:\n", "\n", "http://scikit-learn.org/stable/supervised_learning.html"]}, {"metadata": {}, "cell_type": "markdown", "source": ["# Linear Regression\n"]}, {"metadata": {}, "cell_type": "markdown", "source": ["A first simple approach is to apply a linear model"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["from sklearn import linear_model                \n", "\n", "# Create linear regression object\n", "regr = linear_model.LinearRegression()\n", "\n", "# Train the model using the training sets\n", "regr.fit(X,y)\n", "\n", "# Make predictions using the testing set\n", "y_test_LR = regr.predict(X_test)\n", "\n", "# add a new column to data frame that already exists\n", "w_test_complete['RHOB_pred_LinReg'] = y_test_LR\n", "\n", "my_well = w_test_complete[w_test_complete.well_ID==well_id]\n", "\n", "plt.figure(figsize=(3,10))\n", "plt.plot(my_well.RHOB, my_well.Depth, 'k')\n", "plt.plot(my_well.RHOB_pred_LinReg, my_well.Depth,'r')"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["<div class=\"alert alert-success\">\n", "    <b>Exercise</b>:\n", "     <ul>\n", "      <li>\n", "      Complete the following code to test the different classifiers similar to the Linear Regression case\n", "      </li>\n", "      <p>\n", "    </ul>\n", "</div>"]}, {"metadata": {}, "cell_type": "markdown", "source": ["\n", "# Decision Tree Regressor"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["from sklearn import tree\n", "clf = tree.DecisionTreeRegressor()\n", "\n", "#--!\n", "clf = clf.fit(X, y)\n", "\n", "y_test_DTR = clf.predict(X_test)\n", "#--!"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["# add a new column to data frame that already exists and plot the results\n", "#!--\n", "w_test_complete['RHOB_pred_DTR'] = y_test_DTR\n", "w_test_complete.head()\n", "\n", "\n", "my_well = w_test_complete[w_test_complete.well_ID==well_id]\n", "\n", "plt.figure(figsize=(3,10))\n", "plt.plot(my_well.RHOB, my_well.Depth, 'k')\n", "plt.plot(my_well.RHOB_pred_DTR, my_well.Depth,'r')\n", "#--!"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["# Nearest Neighbours"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["from sklearn.neighbors import KNeighborsRegressor\n", "\n", "nbrs = KNeighborsRegressor()\n", "#!--\n", "nbrs.fit(X, y)\n", "y_test_KNN = nbrs.predict(X_test)\n", "#--!"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["# add a new column to data frame that already exists and plot the results\n", "#!--\n", "w_test_complete['RHOB_pred_KNN'] = y_test_KNN\n", "\n", "my_well = w_test_complete[w_test_complete.well_ID==well_id]\n", "\n", "plt.figure(figsize=(3,10))\n", "plt.plot(my_well.RHOB, my_well.Depth, 'k')\n", "plt.plot(my_well.RHOB_pred_KNN, my_well.Depth,'r')\n", "#--!"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["# Gradient Boosting Ensemble Regressor"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["import numpy as np\n", "from sklearn.metrics import mean_squared_error\n", "from sklearn.ensemble import GradientBoostingRegressor\n", "\n", "#!--\n", "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.05,\n", "    max_depth=5, random_state=0, loss='ls')\n", "\n", "est.fit(X, y)\n", "\n", "y_test_GBT = est.predict(X_test)\n", "w_test_complete['RHOB_pred_GBT'] = y_test_GBT\n", "\n", "my_well = w_test_complete[w_test_complete.well_ID==well_id]\n", "\n", "plt.figure(figsize=(3,10))\n", "plt.plot(my_well.RHOB, my_well.Depth, 'k')\n", "plt.plot(my_well.RHOB_pred_GBT, my_well.Depth,'r')\n", "#--!"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["# Evaluation Metrics"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Although it's good to see how the plots look, a more generalized way to determine how good a model is at predicting data\n"]}, {"metadata": {}, "cell_type": "markdown", "source": ["http://scikit-learn.org/stable/model_selection.html#model-selection"]}, {"metadata": {}, "cell_type": "markdown", "source": ["\"Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. Note that the word \u201cexperiment\u201d is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally.\""]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["from sklearn.model_selection import cross_val_score\n", "\n", "scores = cross_val_score(est, X_test, w_test_complete.RHOB, cv=5, scoring='neg_mean_squared_error')\n", "scores  "], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["## Regression metrics"]}, {"metadata": {}, "cell_type": "markdown", "source": ["[TOP](#top)"]}, {"metadata": {}, "cell_type": "markdown", "source": ["http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics"]}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["from sklearn.metrics import explained_variance_score\n", "print(explained_variance_score(my_well.RHOB, my_well.RHOB_pred_LinReg))  \n", "print(explained_variance_score(my_well.RHOB, my_well.RHOB_pred_DTR))\n", "print(explained_variance_score(my_well.RHOB, my_well.RHOB_pred_KNN))\n"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["from sklearn.metrics import mean_squared_error\n", "print(mean_squared_error(my_well.RHOB, my_well.RHOB_pred_LinReg))  \n", "print(mean_squared_error(my_well.RHOB, my_well.RHOB_pred_DTR))\n", "print(mean_squared_error(my_well.RHOB, my_well.RHOB_pred_KNN))"], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["# Feature Engineering"]}, {"metadata": {}, "cell_type": "markdown", "source": ["What can we do to help our classifier?"]}, {"metadata": {}, "cell_type": "markdown", "source": ["<div class=\"alert alert-success\">\n", "    <b>Exercise</b>:\n", "     <ul>\n", "      <li>\n", "      Create a function using `np.convolve` to smooth a log curve and return the smoothed version to add to the `DataFrame`\n", "      </li>\n", "      <p>\n", "    </ul>\n", "</div>"]}, {"outputs": [], "metadata": {"tags": ["hide"]}, "cell_type": "code", "source": ["#!--\n", "def smooth(y, box_len=10):\n", "    box = np.ones(box_len)/box_len\n", "    y_smooth = np.convolve(y, box, mode='same')\n", "    return y_smooth\n", "#--!"], "execution_count": null}, {"outputs": [], "metadata": {"tags": ["hide"]}, "cell_type": "code", "source": ["w_train.columns"], "execution_count": null}, {"outputs": [], "metadata": {"tags": ["hide"]}, "cell_type": "code", "source": ["w_train[\"s_NPHI\"] = smooth(w_train[\"NPHI\"].values, box_len=50) "], "execution_count": null}, {"outputs": [], "metadata": {"tags": ["hide"]}, "cell_type": "code", "source": ["w_train[\"well_ID\"].unique()"], "execution_count": null}, {"outputs": [], "metadata": {"tags": ["hide"]}, "cell_type": "code", "source": ["idx_test_well = 0"], "execution_count": null}, {"outputs": [], "metadata": {"tags": ["hide"]}, "cell_type": "code", "source": ["plt.plot(w_train[w_train.well_ID == idx_test_well][\"NPHI\"])\n", "plt.plot(w_train[w_train.well_ID == idx_test_well][\"s_NPHI\"])"], "execution_count": null}, {"outputs": [], "metadata": {"tags": ["hide"]}, "cell_type": "code", "source": ["w_test[\"s_NPHI\"] = smooth(w_test[\"NPHI\"].values, box_len=50)\n", "X_test = w_test[['Depth','GR','ILD','NPHI','s_NPHI']].as_matrix()"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["# s_NPHI will be the smoothed array!\n", "X = w_train[['Depth','GR','ILD','NPHI','s_NPHI']].as_matrix()\n", "\n", "#!--\n", "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.05,\n", "    max_depth=5, random_state=0, loss='ls')\n", "\n", "est.fit(X, y)\n", "\n", "y_test_GBT = est.predict(X_test)\n", "\n", "\n", "w_test_complete['RHOB_pred_GBT'] = y_test_GBT\n", "\n", "my_well = w_test_complete[w_test_complete.well_ID==well_id]\n", "\n", "plt.figure(figsize=(3,10))\n", "plt.plot(my_well.RHOB, my_well.Depth, 'k')\n", "plt.plot(my_well.RHOB_pred_GBT, my_well.Depth,'r')\n", "#--!"], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["print(mean_squared_error(my_well.RHOB, my_well.RHOB_pred_GBT)) "], "execution_count": null}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["from sklearn.metrics import mean_squared_error\n", "print(mean_squared_error(my_well.RHOB, my_well.RHOB_pred_GBT))  "], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": ["<hr />\n", "\n", "<p style=\"color:gray\">\u00a92017 Agile Geoscience. Licensed CC-BY.</p>"]}], "nbformat_minor": 2}