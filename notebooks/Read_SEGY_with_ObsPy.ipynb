{"metadata": {"language_info": {"pygments_lexer": "ipython3", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.3", "file_extension": ".py", "nbconvert_exporter": "python"}, "anaconda-cloud": {}, "kernelspec": {"name": "python3", "display_name": "Python [default]", "language": "python"}}, "nbformat": 4, "cells": [{"metadata": {}, "cell_type": "markdown", "source": ["## Read SEG-Y with `obspy`\n", "\n", "Before going any further, you might like to know, [What is SEG-Y?](http://www.agilegeoscience.com/blog/2014/3/26/what-is-seg-y.html). See also the articles in [SubSurfWiki](http://www.subsurfwiki.org/wiki/SEG_Y) and [Wikipedia](https://en.wikipedia.org/wiki/SEG_Y).\n", "\n", "We'll use the [obspy](https://github.com/obspy/obspy) seismology library to read and write SEGY data.\n", "    \n", "Technical SEG-Y documentation:\n", "\n", "* [SEG-Y Rev 1](http://seg.org/Portals/0/SEG/News%20and%20Resources/Technical%20Standards/seg_y_rev1.pdf)\n", "* [SEG-Y Rev 2 proposal](https://www.dropbox.com/s/txrqsfuwo59fjea/SEG-Y%20Rev%202.0%20Draft%20August%202015.pdf?dl=0) and [draft repo](http://community.seg.org/web/technical-standards-committee/documents/-/document_library/view/6062543)"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["ls -l ../data/*.sgy"]}, {"metadata": {}, "cell_type": "markdown", "source": ["## 2D data"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["filename = '../data/HUN00-ALT-01_STK.sgy'"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["from obspy.io.segy.segy import _read_segy\n", "section = _read_segy(filename)\n", "\n", "# OPTIONS\n", "# headonly=True \u2014\u00a0only reads the header info, then you can index in on-the-fly.\n", "# unpack_headers=True \u2014 slows you down here and isn't really required."]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["data = np.vstack([t.data for t in section.traces])"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["plt.figure(figsize=(16,8))\n", "plt.imshow(data.T, cmap=\"Greys\")\n", "plt.colorbar(shrink=0.5)\n", "plt.show()"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["section.traces[0]"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["section.textual_file_header"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Aargh... \n", "\n", "OK, fine, we'll reformat this."]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["def chunk(string, width=80):\n", "    try:\n", "        # Make sure we don't have a ``bytes`` object.\n", "        string = string.decode()\n", "    except:\n", "        # String is already a string, carry on.\n", "        pass\n", "    lines = int(np.ceil(len(string) / width))\n", "    result = ''\n", "    for i in range(lines):\n", "        line = string[i*width:i*width+width]\n", "        result += line + (width-len(line))*' ' + '\\n'\n", "    return result\n", "\n", "s = section.textual_file_header.decode()\n", "print(chunk(s))"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["section.traces[0]"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["t = section.traces[0]\n", "\n", "t.npts"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["t.header"]}, {"metadata": {}, "cell_type": "markdown", "source": ["## 3D data\n", "\n", "Either use the small volume, or **[get the large dataset from Agile's S3 bucket](https://s3.amazonaws.com/agilegeo/Penobscot_0-1000ms.sgy.gz)**"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["#filename = '../data/F3_very_small.sgy'\n", "filename = '../data/Penobscot_0-1000ms.sgy'"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["from obspy.io.segy.segy import _read_segy\n", "\n", "raw = _read_segy(filename)"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["data = np.vstack([t.data for t in raw.traces])"]}, {"metadata": {}, "cell_type": "markdown", "source": ["I happen to know that the shape of this dataset is 601 &times; 481."]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["_, t = data.shape\n", "seismic = data.reshape((601, 481, t))"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Note that we don't actually need to know the last dimension, if we already have two of the three dimensions. `np.reshape()` can compute it for us on the fly:"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["seismic = data.reshape((601, 481, -1))"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Plot the result..."]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["clip = np.percentile(seismic, 99)\n", "fig = plt.figure(figsize=(12,6))\n", "ax = fig.add_subplot(111)\n", "plt.imshow(seismic[100,:,:].T, cmap=\"Greys\", vmin=-clip, vmax=clip)\n", "plt.colorbar(label=\"Amplitude\", shrink=0.8)\n", "ax.set_xlabel(\"Trace number\")\n", "ax.set_ylabel(\"Time sample\")\n", "plt.show()"]}, {"metadata": {}, "cell_type": "markdown", "source": ["<hr />\n", "\n", "<div>\n", "<img src=\"https://avatars1.githubusercontent.com/u/1692321?s=50\"><p style=\"text-align:center\">\u00a9 Agile Geoscience 2016</p>\n", "</div>"]}], "nbformat_minor": 1}