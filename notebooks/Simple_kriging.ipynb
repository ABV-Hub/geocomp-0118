{"metadata": {"language_info": {"pygments_lexer": "ipython3", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.3", "file_extension": ".py", "nbconvert_exporter": "python"}, "anaconda-cloud": {}, "kernelspec": {"name": "python3", "display_name": "Python [default]", "language": "python"}}, "nbformat": 4, "cells": [{"metadata": {}, "cell_type": "markdown", "source": ["# Simple kriging in Python\n", "\n", "This follows a tutorial and code by Connor Johnson, in [his blog post](http://connor-johnson.com/2014/03/20/simple-kriging-in-python/). It is openly licensed under the MIT license.\n", "\n", "Some more geostatistics resources:\n", "\n", "- More from Connor Johnson: https://github.com/cjohnson318/geostatsmodels\n", "- Another library: https://github.com/whimian/pyGeoStatistics\n", "- HPGL: https://github.com/hpgl/hpgl\n", "- From Clayton Deutsch's lab: http://www.ccgalberta.com/pygeostat/welcome.html\n", "- Following a scikit-learn API: https://pypi.python.org/pypi/scikit-gstat/0.1.6\n", "\n", "## What is kriging?\n", "\n", "Kriging is a set of techniques for interpolation. It differs from other interpolation techniques in that it sacrifices smoothness for the integrity of sampled points. Most interpolation techniques will over or undershoot the value of the function at sampled locations, but kriging honors those measurements and keeps them fixed.\n", "\n", "## Data\n", "\n", "We use the data from **Geoff Bohling** at the Kansas Geological Survey. [Click here](http://people.ku.edu/~gbohling/geostats/index.html) then look for \"My tutorial on reservoir modeling...\". I'm using the `ZoneA.dat` file."]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["import numpy as np\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "from scipy.spatial.distance import pdist, squareform\n", " \n", "with open( '../data/ZoneA.dat', 'r') as f:\n", "    z = f.readlines()\n", "z = [ i.strip().split() for i in z[10:] ]\n", "z = np.array( z, dtype=np.float )\n", "z = pd.DataFrame( z, columns=['x', 'y', 'thk', 'por', 'perm', 'lperm', 'lpermp', 'lpermr'] )"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["fig, ax = plt.subplots()\n", "ax.scatter( z.x, z.y, c=z.por)\n", "ax.set_aspect(1)\n", "plt.xlim(-1500,22000)\n", "plt.ylim(-1500,17500)\n", "plt.xlabel('Easting [m]')\n", "plt.ylabel('Northing [m]')\n", "plt.title('Porosity %')\n", "plt.show()"]}, {"metadata": {}, "cell_type": "markdown", "source": ["## The semivariogram\n", "\n", "According to Connor, these formulations follow Geoff Bohling's and Clayton Deutsch's work.\n", "\n", "The semivariogram encodes data about spatial variance over the region at a given distance or lag. We generally expect data points that are close together spatially to share other characteristics, and we expect points that are separated by greater distances to have lesser correlation. The semivariogram allows us to model the similarity points in a field as a function of distance. The semivariogram is given by,\n", "\n", "$$ \\hat{\\gamma}(h) = \\dfrac{1}{2N(h)} \\displaystyle \\sum_{N(h)} ( z_{i} - z_{j} )^{2} $$\n", "\n", "Here, h is distance specified by the user, and z_{i} and z_{j} are two points that are separated spatially by h. The N(h) term is the number of points we have that are separated by the distance h. The semivariogram then is the sum of squared differences between values separated by a distance h. As an aside, contrast this with the formulation for variance,\n", "\n", "$$ s = \\dfrac{1}{N-1} \\displaystyle \\sum_{k=1}^{N} (z_{k} - \\hat{\\mu} )^{2} $$ \n", "\n", "Here, $N$ is the number of data points, $\\hat{\\mu}$ is the sample mean, and $z_{k}$ is a data point. For sample variance, we are taking the squared difference between data points and the mean, and in the semivariogram we are taking the squared difference between data points separated by distance $h$. We can write some functions to calculate the semivariogram at one lag, and then at multiple lags as follows."]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["def SVh(P, h, bw):\n", "    '''\n", "    Experimental semivariogram for a single lag.\n", "    '''\n", "    dists = squareform(pdist(P[:,:2]))\n", "    N = dists.shape[0]\n", "    Z = list()\n", "    for i in range(N):\n", "        for j in range(i+1,N):\n", "            if( dists[i,j] >= h-bw )and( dists[i,j] <= h+bw ):\n", "                Z.append(( P[i,2] - P[j,2])**2)\n", "    return np.sum(Z) / (2.0 * len( Z ))\n", " \n", "def SV(P, hs, bw):\n", "    '''\n", "    Experimental variogram for a collection of lags.\n", "    '''\n", "    sv = list()\n", "    for h in hs:\n", "        sv.append( SVh( P, h, bw ) )\n", "    sv = [ [ hs[i], sv[i] ] for i in range( len( hs ) ) if sv[i] > 0 ]\n", "    return np.array( sv ).T"]}, {"metadata": {}, "cell_type": "markdown", "source": ["The C() function is the covariance function, and will be used later. Let us now calculate and plot the semivariogram,"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["def C(P, h, bw):\n", "    '''\n", "    Calculate the sill.\n", "    '''\n", "    c0 = np.var( P[:,2] )\n", "    if h == 0:\n", "        return c0\n", "    return c0 - SVh( P, h, bw )"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["# Part of our data set recording porosity.\n", "P = np.array(z[['x', 'y', 'por']])\n", "\n", "# Bandwidth, plus or minus 250 meters.\n", "bw = 500\n", "\n", "# Lags in 500 meter increments from zero to 10,000.\n", "hs = np.arange(0, 10500, bw)\n", "sv = SV( P, hs, bw )\n", "\n", "# Make a plot.\n", "plt.plot( sv[0], sv[1], '.-' )\n", "plt.xlabel('Lag [m]')\n", "plt.ylabel('Semivariance')\n", "plt.title('Sample semivariogram') ;\n", "plt.show()"]}, {"metadata": {}, "cell_type": "markdown", "source": ["## Modeling\n", "\n", "Now that we\u2019ve calculated the semivariogram, we will need to fit a model to the data. There are three popular models, the spherical, exponential, and the Gaussian. Here, we\u2019ll implement the spherical model. First, we will present a function named opt() for determining the optimal value a for the spherical model."]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["def opt(func, x, y, C0, parameterRange=None, meshSize=1000):\n", "    if parameterRange == None:\n", "        parameterRange = [x[1], x[-1]]\n", "    mse = np.zeros(meshSize)\n", "    a = np.linspace(parameterRange[0], parameterRange[1], meshSize)\n", "    for i in range(meshSize):\n", "        mse[i] = np.mean((y - func(x, a[i], C0))**2.0)\n", "    return a[mse.argmin()]"]}, {"metadata": {}, "cell_type": "markdown", "source": ["The opt() function finds the optimal parameter for fitting a spherical model to the semivariogram data. The spherical model is given by the function spherical(). On the last line we see that spherical() returns itself in a map() function, which seems odd. The idea is that the input h can be a single float value, or list or NumPy array of floats. If h is a single value, then line 9 is called. If h is a list or an array (an iterable) then line 17 is called, which applies line 9 to each value of h."]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["def spherical(h, a, C0):\n", "    '''\n", "    Spherical model of the semivariogram\n", "    '''\n", "    # If h is a scalar:\n", "    if np.ndim(h) == 0:\n", "        # Calculate the spherical function.\n", "        if h <= a:\n", "            return C0 * ( 1.5*h/a - 0.5*(h/a)**3.0 )\n", "        else:\n", "            return C0\n", "    else:\n", "        # Calculate the spherical function for all elements.\n", "        a = np.ones(h.size) * a\n", "        C0 = np.ones(h.size) * C0\n", "        return np.array(list(map(spherical, h, a, C0)))"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Next, `cvmodel()` fits a model to the semivariogram data and returns a covariance method named `covfct()`."]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["def cvmodel(P, model, hs, bw):\n", "    '''\n", "    Input:  (P)      ndarray, data\n", "            (model)  modeling function\n", "                      - spherical\n", "                      - exponential\n", "                      - gaussian\n", "            (hs)     distances\n", "            (bw)     bandwidth\n", "    Output: (covfct) function modeling the covariance\n", "    '''\n", "    # Calculate the semivariogram.\n", "    sv = SV(P, hs, bw)\n", "    # Calculate the sill.\n", "    C0 = C(P, hs[0], bw)\n", "    # Calculate the optimal parameters.\n", "    param = opt(model, sv[0], sv[1], C0)\n", "    # Return a covariance function.\n", "    return lambda h, a=param: C0 - model(h, a, C0)"]}, {"metadata": {}, "cell_type": "markdown", "source": ["At this point we\u2019ll plot our model and see if it represents our data well."]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["sp = cvmodel(P, model=spherical, hs=np.arange(0, 10500, 500), bw=500)\n", "\n", "plt.plot( sv[0], sv[1], '.-' )\n", "plt.plot( sv[0], sp( sv[0] ) ) ;\n", "plt.title('Spherical Model')\n", "plt.ylabel('Semivariance')\n", "plt.xlabel('Lag [m]')\n", "plt.show()"]}, {"metadata": {}, "cell_type": "markdown", "source": ["## Kriging\n", "\n", "Now that we have a model for the semivariogram, we can write a function to perform the kriging. The fundamental relationship is a matrix equation,\n", "\n", "$$ K \\lambda = k \\Rightarrow \\lambda = K^{-1} k $$\n", "\n", "Here, $K$ is a matrix of covariances calculated using the spherical model, $\\lambda$ is a vector of simple kriging weights, and $k$ is the vector of covariances between the data points and an unsampled point. Our kriging function takes the data set `P`, the `model`, the distances `hs`, the bandwidth `bw`, the coordinates of the unsampled point `u`, and the number of surrounding points `N` to use in the calculation."]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["def krige(P, model, hs, bw, u, N):\n", "    '''\n", "    Input  (P)     ndarray, data\n", "           (model) modeling function\n", "                    - spherical\n", "                    - exponential\n", "                    - gaussian\n", "           (hs)    kriging distances\n", "           (bw)    kriging bandwidth\n", "           (u)     unsampled point\n", "           (N)     number of neighboring\n", "                   points to consider\n", "    '''\n", " \n", "    # covariance function\n", "    covfct = cvmodel(P, model, hs, bw)\n", "    # mean of the variable\n", "    mu = np.mean(P[:,2])\n", " \n", "    # distance between u and each data point in P\n", "    d = np.sqrt((P[:,0]-u[0])**2.0 + (P[:,1]-u[1])**2.0)\n", "    # add these distances to P\n", "    P = np.vstack(( P.T, d )).T\n", "    # sort P by these distances\n", "    # take the first N of them\n", "    P = P[d.argsort()[:N]]\n", " \n", "    # apply the covariance model to the distances\n", "    k = covfct( P[:,3] )\n", "    # cast as a matrix\n", "    k = np.matrix( k ).T\n", " \n", "    # form a matrix of distances between existing data points\n", "    K = squareform( pdist( P[:,:2] ) )\n", "    # apply the covariance model to these distances\n", "    K = covfct( K.ravel() )\n", "    # re-cast as a NumPy array -- thanks M.L.\n", "    K = np.array( K )\n", "    # reshape into an array\n", "    K = K.reshape(N,N)\n", "    # cast as a matrix\n", "    K = np.matrix( K )\n", " \n", "    # calculate the kriging weights\n", "    weights = np.linalg.inv( K ) * k\n", "    weights = np.array( weights )\n", " \n", "    # calculate the residuals\n", "    residuals = P[:,2] - mu\n", " \n", "    # calculate the estimation\n", "    estimation = np.dot( weights.T, residuals ) + mu\n", " \n", "    return float( estimation )"]}, {"metadata": {}, "cell_type": "markdown", "source": ["## Estimation\n", "\n", "Here, we\u2019ll calculate the kriging estimate at a number of unsampled points."]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["P[:,0].min(), P[:,0].max(), P[:,1].min(), P[:,1].max()"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["X0, X1 = 0, 20000\n", "Y0, Y1 = 0, 16000\n", "\n", "# Define the number of grid cells over which to make estimates.\n", "# TODO: Vectorize this. I'll try numba/jit but I don't think it'll help.\n", "# I think it can be vectorized with np.mgrid (better than np.meshgrid)\n", "\n", "# Many points:\n", "x, y = 100, 80\n", "\n", "# Fewer points:\n", "x, y = 50, 40\n", "\n", "dx, dy = (X1-X0) / x, (Y1-Y0) / y\n", "\n", "def stepwise(x, y):\n", "    Z = np.zeros((y, x))\n", "    \n", "    for i in range(y):\n", "        print(i, end=' ')\n", "        for j in range(x):\n", "            Z[i, j] = krige(P, model=spherical, hs=hs, bw=bw, u=(dy*j, dx*i), N=16)\n", "            \n", "    return Z"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["# THIS IS SLOW\n", "# Z = stepwise(x, y)"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["Z"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["extent = [X0, X1, Y0, Y1]\n", "\n", "plt.imshow(Z, origin='lower', interpolation='none', extent=extent)\n", "plt.scatter(z.x, z.y, s=2, c='w')\n", " \n", "plt.show()"]}, {"metadata": {}, "cell_type": "markdown", "source": ["## Comparison to 2D Gaussian process regression\n", "\n", "This needs writing.\n", "\n", "https://stackoverflow.com/questions/41572058/how-to-correctly-use-scikit-learns-gaussian-process-for-a-2d-inputs-1d-output/43409379"]}, {"metadata": {}, "outputs": [], "execution_count": null, "cell_type": "code", "source": []}], "nbformat_minor": 1}