{"nbformat": 4, "nbformat_minor": 2, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "version": "3.5.4", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py"}, "anaconda-cloud": {}}, "cells": [{"source": ["# Intro to machine learning - Scikit-Learn\n", "\n", "We'll explore the Pandas and Scikit Learn packages for simple machine learning tasks using geoscience data examples. After this day, students will have a good overview of how to look at large datasets and solve problems with state-of-the-art machine learning tools.\n", "\n", "- Machine learning concepts\n", "- What is it that you\u2019re trying to solve? How can machine learning help?\n", "- What's the difference between supervised and unsupervised methods?\n", "- What's the difference between classification and regression?\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["<img src=\"../data/ML_loop.png\"></img>"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Machine learning concepts"], "cell_type": "markdown", "metadata": {}}, {"source": ["### The machine learning iterative loop\n", "- Data \u2014 Getting the data. How to load it and put it in an `array` and/or `DataFrame`\n", "- Processing \u2014 data exploration, inspection, cleaning, and feature engineering.\n", "- Model \u2013 What is a model? Training a Scikit-Learn model.\n", "- Results \u2013 assessing quality and performance metrics (accuracy, recall, F1, confusion matrices)\n", "- Repeat \u2013 What can we do to improve performance?"], "cell_type": "markdown", "metadata": {}}, {"source": ["### Data management for machine learning\n", "- DataFrames: A new way to look at well logs.\n", "- DataFrames vs arrays.\n", "\n", "\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Basic Pandas"], "cell_type": "markdown", "metadata": {}}, {"source": ["Introduces the concept of a `DataFrame` in Python. If you're familiar with R, it's pretty much the same idea! Useful cheat sheet [here](https://www.datacamp.com/community/blog/pandas-cheat-sheet-python#gs.59HV6BY)"], "cell_type": "markdown", "metadata": {}}, {"source": ["The main purpose of Pandas is to allow easy manipulation of data in tabular form. Perhaps the most important idea that makes Pandas great for data science, is that it will always preserve **alignment** between data and labels."], "cell_type": "markdown", "metadata": {}}, {"source": ["import pandas as pd\n", "import numpy as np"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["The most common data structure in Pandas is the `DataFrame`. A 2D structure that can hold various types of Python objects indexed by an `index` array (or multiple `index` arrays). Columns are usually labelled as well using strings.\n", "\n", "An easy way to think about a `DataFrame` is if you imagine it as an Excel spreadsheet.\n", "\n", "Let's define one using a numpy array:"], "cell_type": "markdown", "metadata": {}}, {"source": ["arr =  [[1.23, 'sandstone'],\n", "        [3.654, 'limestone'],\n", "        [0.998, 'shale']]\n", "arr"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Make a `DataFrame` from `arr`"], "cell_type": "markdown", "metadata": {}}, {"source": ["df = pd.DataFrame(arr, columns=['param1', 'lithology'])\n", "df"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["df.loc[df['param1'] > 1,'param1']\n"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Accessing the data is a bit more complex than in the numpy array cases but for good reasons"], "cell_type": "markdown", "metadata": {}}, {"source": ["df.loc[1,'lithology'] # .loc[index, column]"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Add more data (row wise)"], "cell_type": "markdown", "metadata": {}}, {"source": ["df.loc[3] = [5.6, 'shale']\n", "df"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Add data (column wise) specifying the index locations"], "cell_type": "markdown", "metadata": {}}, {"source": ["df.loc[0:2, 'one_more_column'] = [6,7,8]\n", "df"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Add a new column with a \"complete\" list, array or series"], "cell_type": "markdown", "metadata": {}}, {"source": ["df['second_new_colum'] = [\"x\",\"y\",\"z\",\"a\"]\n", "df"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Pandas also reads files from disk in tabular form ([here](http://pandas.pydata.org/pandas-docs/version/0.20/io.html)'s a list of all the formats that it can read and write). A very common one is CSV, so let's load one!"], "cell_type": "markdown", "metadata": {}}, {"source": ["df = pd.read_csv(\"../data/2016_ML_contest_training_data.csv\")\n", "df.head()"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["# Inspecting the `DataFrame`"], "cell_type": "markdown", "metadata": {}}, {"source": ["Using the `DataFrame` with well log information loaded before, we can make a summary using the `describe()` method of the `DataFrame` object"], "cell_type": "markdown", "metadata": {}}, {"source": ["df.describe()"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["df = df.dropna()"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["## Adding more data to the `DataFrame`"], "cell_type": "markdown", "metadata": {}}, {"source": ["def rhob(phi_rhob, Rho_matrix= 2650.0, Rho_fluid=1000.0):\n", "    \"\"\"\n", "    Rho_matrix (sandstone) : 2.65 g/cc\n", "    Rho_matrix (Limestome): 2.71 g/cc\n", "    Rho_matrix (Dolomite): 2.876 g/cc\n", "    Rho_matrix (Anyhydrite): 2.977 g/cc\n", "    Rho_matrix (Salt): 2.032 g/cc\n", "\n", "    Rho_fluid (fresh water): 1.0 g/cc (is this more mud-like?)\n", "    Rho_fluid (salt water): 1.1 g/cc\n", "    see wiki.aapg.org/Density-neutron_log_porosity\n", "    returns density porosity log \"\"\"\n", "    \n", "    return Rho_matrix*(1 - phi_rhob) + Rho_fluid*phi_rhob\n"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["phi_rhob = 2*(df.PHIND/100)/(1 - df.DeltaPHI/100) - df.DeltaPHI/100\n", "calc_RHOB = rhob(phi_rhob)\n", "df['RHOB'] = calc_RHOB"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["df.describe()"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["We can define a Python dictionary to relate facies with the integer label on the `DataFrame`"], "cell_type": "markdown", "metadata": {}}, {"source": ["facies_dict = {1:'sandstone', 2:'c_siltstone', 3:'f_siltstone', 4:'marine_silt_shale',\n", "               5:'mudstone', 6:'wackestone', 7:'dolomite', 8:'packstone', 9:'bafflestone'}"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Let's add a new column with the name version of the facies"], "cell_type": "markdown", "metadata": {}}, {"source": ["df[\"s_Facies\"] = df.Facies.map(lambda x: facies_dict[x])"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["df.head()"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["## Visual exploration of the data"], "cell_type": "markdown", "metadata": {}}, {"source": ["We can easily visualize the properties of each facies and how they compare using a `PairPlot`. The library `seaborn` integrates with matplotlib to make these kind of plots easily."], "cell_type": "markdown", "metadata": {}}, {"source": ["import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "g = sns.PairGrid(df, hue=\"s_Facies\", vars=['GR','RHOB','PE','ILD_log10'], size=4)\n", "\n", "g.map_upper(plt.scatter,**dict(alpha=0.4))  \n", "g.map_lower(plt.scatter,**dict(alpha=0.4))\n", "g.map_diag(plt.hist,**dict(bins=20))  \n", "g.add_legend()"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["It is very clear that it's hard to separate these facies in feature space. Let's just select a couple of facies and using Pandas, select the rows in the `DataFrame` that contain information about those facies "], "cell_type": "markdown", "metadata": {}}, {"source": ["selected = ['f_siltstone', 'bafflestone', 'wackestone']\n", "\n", "dfs = pd.concat(list(map(lambda x: df[df.s_Facies == x], selected)))\n", "\n", "g = sns.PairGrid(dfs, hue=\"s_Facies\", vars=['GR','RHOB','PE','ILD_log10'], size=4)  \n", "g.map_upper(plt.scatter, alpha=0.4)\n", "g.map_lower(plt.scatter, alpha=0.4)\n", "g.map_diag(plt.hist,**dict(bins=20))  \n", "g.add_legend()"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["---\n", "# Feature engineering"], "cell_type": "markdown", "metadata": {}}, {"source": ["Add PCA components? Average logs as function of Depth? ..."], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "# Scikit-learn classifiers\n", "\n", "Let's create a model that classifies between those three classes\n", "\n", "### For a classifier comparison check the source code [here](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n", "\n", "<img src=\"../data/ML_classifier_comparison_sklearn.png\"></img>\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["*Choosing the right estimator:* Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.\n", "Different estimators are better suited for different types of data and different problems."], "cell_type": "markdown", "metadata": {}}, {"source": ["# Make X and y\n", "X = dfs[['GR','RHOB','PE','ILD_log10']].as_matrix()\n", "y = dfs['s_Facies'].values"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Some methods expect the data to be normalized. It's sometimes a good idea of normalizing it no matter which method you try"], "cell_type": "markdown", "metadata": {}}, {"source": ["from sklearn.preprocessing import StandardScaler\n", "\n", "scaler = StandardScaler()\n", "X = scaler.fit_transform(X)"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["plt.scatter(X[:,0], X[:,1], c=dfs['Facies'].values)"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Split the data into a training set and a test set. **This is a key step in the process**"], "cell_type": "markdown", "metadata": {}}, {"source": ["from sklearn.model_selection import train_test_split\n", "\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["A fairly common method for classifying data is to use the _k-nearest neighbors algorithm_. The label of the object in question is determined by the neighbouring data points in the feature space used. Its most important parameter, `k`, is the number of neighbors you include to make a membership decision."], "cell_type": "markdown", "metadata": {}}, {"source": ["from sklearn.neighbors import KNeighborsClassifier"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["The next block is all you need to train a classifier model!"], "cell_type": "markdown", "metadata": {}}, {"source": ["clf = KNeighborsClassifier()\n", "clf.fit(X_train, y_train)"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Before we can move on to make predictions we need to create validation routines to make sure that the model we trained is _good_ and produces reasonble results. The most basic test is to look at how many good predictions we would make if we predict on our `Test` data."], "cell_type": "markdown", "metadata": {}}, {"source": ["score = clf.score(X_test, y_test)\n", "print(\"The precision is {}%\".format(np.round(score*100)))"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["This scoring is one of the _metrics_ we can use to check the quality of the predictions. There are a large number of different metrics and depending on your data and problem you may need to find the one that adjusts better to your needs. Typically, a more robust metric that is often used is called `F1`. It combines the `precision` score and a `recall` score (how many true positive predictions were made). Scikit-learn gives a nice summary of these three metrics using `classification_report`."], "cell_type": "markdown", "metadata": {}}, {"source": ["from sklearn.metrics import classification_report\n", "print(classification_report(y_test, clf.predict(X_test), digits=3))"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Depending on you requirements, this results might be good enough to deploy this model and use it on a \"Machine Learning Pipeline\" product but it is often not the best model you can get. Each method has a set of parameters (also known as _hyperparameters_) that can be tweaked to tune the training.\n", "\n", "For the `KNeighborsClassifier` there are a few of these parameters:"], "cell_type": "markdown", "metadata": {}}, {"source": ["KNeighborsClassifier()"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["For this particular method, the most important parameter to adjust is `n_neighbors` (it's the `K` in the `KNeighborsClassifier`!). Unfortunately, there's no rule that tells you what's the optimal value of `k`. To overcome this we can train many models with different values of `k` and compare the results of classifications applied to the _Test_ data."], "cell_type": "markdown", "metadata": {}}, {"source": ["nns = np.arange(1,60,2) # Generated array of values of k to try"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Loop over each value in `nns` and store the `F1 Score`"], "cell_type": "markdown", "metadata": {}}, {"source": ["from sklearn.metrics import f1_score\n", "\n", "acc = []\n", "for n in nns:\n", "    clf = KNeighborsClassifier(n)\n", "    clf.fit(X_train, y_train)\n", "    y_pred = clf.predict(X_test)\n", "    score = f1_score(y_pred, y_test, average='weighted')\n", "    acc.append(score)"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["What value of `n` gives us the best result?"], "cell_type": "markdown", "metadata": {}}, {"source": ["plt.plot(nns,acc)\n", "_ = plt.xlabel('Number of neighbors')\n", "_ = plt.ylabel('F1 Score')"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["<div class=\"alert alert-success\">\n", "<b>Exercise</b>:\n", "<ul>\n", "</ul>\n", "</div>"], "cell_type": "markdown", "metadata": {"tags": ["exe"]}}, {"source": ["## More methods to train models!"], "cell_type": "markdown", "metadata": {}}, {"source": ["Let's pick 3 different classifiers to train different models and then compare how well they perform"], "cell_type": "markdown", "metadata": {}}, {"source": ["from sklearn.svm import SVC\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.neural_network import MLPClassifier"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["classifiers = [\n", "    SVC(),\n", "    RandomForestClassifier(),\n", "    MLPClassifier()\n", "    ]\n", "\n", "names = [\"Linear SVM\", \"RandomForest\", \"Neural Network\"]"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["classifiers"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["Let's iterate over these classifiers and print common metrics to evaluate the performance of each model using the testing dataset we defined before"], "cell_type": "markdown", "metadata": {}}, {"source": ["# iterate over classifiers\n", "for name, clf in zip(names, classifiers):\n", "    clf.fit(X_train, y_train)\n", "    score = clf.score(X_test, y_test)\n", "    print(\"{:12} {}\".format(name,\"-\"*15))\n", "    print(classification_report(y_test, clf.predict(X_test), digits=3))"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["<div class=\"alert alert-success\">\n", "<b>Exercise</b>:\n", "<ul>\n", "</ul>\n", "</div>"], "cell_type": "markdown", "metadata": {"tags": ["exe"]}}, {"source": ["# Parameter selection"], "cell_type": "markdown", "metadata": {}}, {"source": ["Many of the models can be improved (or worsened) by changing the parameters that internally make the method work. It's always a good idea to check the documentation of each model (e.g. RandomForestClassifier [docs](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)). This process is usually called _hyperparameter tuning_."], "cell_type": "markdown", "metadata": {}}, {"source": ["Scikit-learn offers a simple way to test different parameters for each model through a function called `GridSearchCV`"], "cell_type": "markdown", "metadata": {}}, {"source": ["from sklearn.model_selection import GridSearchCV\n", "from sklearn.metrics import make_scorer as msc\n", "\n", "# Select the parameters and values for each one to test\n", "parameters = {'n_estimators':np.arange(1,100,5),\n", "              'max_depth':np.arange(1,50,5)}\n", "\n", "rfc = RandomForestClassifier()\n", "\n", "clf = GridSearchCV(rfc, parameters, scoring = msc(f1_score,**{'average':'weighted'}), cv=3, n_jobs=8)\n", "\n", "clf.fit(X_train, y_train)"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["How does the parameter space look like with respect to the score of the classifier?"], "cell_type": "markdown", "metadata": {}}, {"source": ["scores = clf.cv_results_['mean_test_score']\n", "max_depths = clf.cv_results_[\"param_max_depth\"].data.astype(int)\n", "n_estimators = clf.cv_results_[\"param_n_estimators\"].data.astype(int)"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["X_size = len(np.unique(max_depths))\n", "Y_size = len(np.unique(n_estimators))\n", "X = max_depths.reshape((X_size, Y_size))\n", "Y = n_estimators.reshape((X_size, Y_size))\n", "Z = scores.reshape((X_size, Y_size))"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["import scipy.interpolate\n", "\n", "# Set up a regular grid of interpolation points\n", "xi, yi = np.linspace(X.min(), X.max(), 100), np.linspace(Y.min(), Y.max(), 100)\n", "xi, yi = np.meshgrid(xi, yi)\n", "\n", "# Interpolate\n", "rbf = scipy.interpolate.Rbf(X, Y, Z, function='linear')\n", "zi = rbf(xi, yi)\n", "\n", "plt.imshow(zi, vmin=0.8, vmax=Z.max(), origin='lower',\n", "           extent=[X.min(), X.max(), Y.min(), Y.max()], aspect=X.max()/Y.max())\n", "# plt.scatter(X, Y, c=Z)\n", "plt.colorbar()\n", "\n", "_ = plt.ylabel('n_estimators')\n", "_ = plt.xlabel('max_depth')"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["`clf` can now tell us the best parameters to use with our `RandomForestClassifier`"], "cell_type": "markdown", "metadata": {}}, {"source": ["clf.best_params_"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["clf.best_estimator_"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["The nice thing about `scikit-learn`'s methods is that they're all consistent and behave in the same way. Notice how`GridSearchCV` was `.fit()`. That means that we can use it to `.predict()` and it will automatically use the best set of parameters!"], "cell_type": "markdown", "metadata": {}}, {"source": ["y_pred = clf.predict(X_test)\n", "print(classification_report(y_test, y_pred, digits=3))"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["It's also helpful to summarize the prediction tests using a [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix). Scikit-learn has a function for that!"], "cell_type": "markdown", "metadata": {}}, {"source": ["from sklearn.metrics import confusion_matrix\n", "\n", "confusion_matrix(y_test, y_pred)"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["But as you can see, it's not very clear... What does each row/column represent? We can help a bit:"], "cell_type": "markdown", "metadata": {}}, {"source": ["# itertoools is a standard library for all kinds of handy iterator manipulation\n", "import itertools\n", "\n", "# Compute confusion matrix\n", "cnf_matrix = confusion_matrix(y_test, y_pred)\n", "\n", "title = 'Confusion matrix'\n", "cmap = plt.cm.Reds\n", "\n", "# Plot non-normalized confusion matrix\n", "plt.figure()\n", "plt.imshow(cnf_matrix, interpolation='nearest', cmap=cmap)\n", "plt.title(title)\n", "plt.colorbar()\n", "tick_marks = np.arange(len(selected))\n", "plt.xticks(tick_marks, selected, rotation=45)\n", "plt.yticks(tick_marks, selected)\n", "\n", "# Print the support numbers inside the plot\n", "thresh = cnf_matrix.max() / 2.\n", "for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n", "    plt.text(j, i, format(cnf_matrix[i, j], 'd'),\n", "             horizontalalignment=\"center\",\n", "             color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n", "\n", "plt.tight_layout()\n", "_ = plt.ylabel('True label')\n", "_ = plt.xlabel('Predicted label')"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["<div class=\"alert alert-success\">\n", "<b>Exercise</b>:\n", "<ul>\n", "</ul>\n", "</div>"], "cell_type": "markdown", "metadata": {"tags": ["ex"]}}, {"source": ["from sklearn.externals import joblib\n", "joblib.dump(clf, 'facies_model.pkl')"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["How do you load a saved model?"], "cell_type": "markdown", "metadata": {}}, {"source": ["clf = joblib.load('facies_model.pkl')"], "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": []}, {"source": ["---\n", "# Where to go next?\n", "\n", "- More data!\n", "- [XGBoost](https://xgboost.readthedocs.io/en/latest/)\n", "- [LightGBM](https://github.com/Microsoft/LightGBM)\n", "- If you want to get started on Neural Networks, [Keras](https://keras.io/) provides a scikit-learn type of experience"], "cell_type": "markdown", "metadata": {}}, {"source": ["### Paper with classifier comparison ([link](https://arxiv.org/abs/1708.05070))\n", "\n", "<img src=\"../data/model_performance.jpg\"></img>"], "cell_type": "markdown", "metadata": {}}, {"source": ["## The Data Science Hierarchy of Needs ([article](https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007))\n", "\n", "<img src=\"../data/the_ai_hierarchy_of_needs.png\"></img>"], "cell_type": "markdown", "metadata": {}}, {"source": ["<hr />\n", "\n", "<p style=\"color:gray\">\u00a92017 Agile Geoscience. Licensed CC-BY.</p>"], "cell_type": "markdown", "metadata": {}}]}